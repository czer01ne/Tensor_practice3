{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception-v3 network using slim\n",
    "\n",
    "* Prerequisites\n",
    "  * `git clone https://github.com/tensorflow/models.git`\n",
    "  * [Pretrained models](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models)\n",
    "* inception-v3를 직접 사용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"$HOME/models/research/slim/\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Inception-v3 graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nets import inception_v3\n",
    "from tensorflow.contrib.slim.python.slim.nets import inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, [None, 299, 299, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with slim.arg_scope(inception_v3.inception_v3_arg_scope()):\n",
    "  logits, end_points = inception_v3.inception_v3(inputs,\n",
    "                                                 num_classes=1001,\n",
    "                                                 is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in end_points:\n",
    "  print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  writer = tf.summary.FileWriter(\"./graphs/09.inception_v3\", sess.graph)\n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Inception-v3 checkpoint: \n",
    "\n",
    "```\n",
    "$ CHECKPOINT_DIR='../checkpoints'\n",
    "$ mkdir ${CHECKPOINT_DIR}\n",
    "$ wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\n",
    "$ tar -xvf inception_v3_2016_08_28.tar.gz\n",
    "$ mv inception_v3_2016_08_28.tar.gz ${CHECKPOINT_DIR}\n",
    "$ rm inception_v3_2016_08_28.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Inception-v3 checkpoint: \n",
    "# if you already have a inception_v3.ckpt then skip and comment below commands\n",
    "#!mkdir ../checkpoints\n",
    "#!wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\n",
    "#!tar -xvf inception_v3_2016_08_28.tar.gz\n",
    "#!mv inception_v3.ckpt ../checkpoints\n",
    "#!rm inception_v3_2016_08_28.tar.gz\n",
    "#print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Inception_v3 weights using `tf.saver.restore`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_v3_preprocessing(image):\n",
    "  \"\"\"inceptino v3 image preprocessing\n",
    "\n",
    "  Args:\n",
    "    image (PIL image): image with shape [height, width, channels]\n",
    "    \n",
    "  Returns:\n",
    "    image (np.int32): inception_v3 preprocessed image with shape [224, 224, 3]\n",
    "  \"\"\"\n",
    "  image = image.resize((299, 299))\n",
    "  image = np.asarray(image)\n",
    "  image = image.astype(np.float32)\n",
    "  image = image / 127.5 - 1.0\n",
    "  \n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_image = Image.open('../input_data/cat2.jpg')\n",
    "my_image = inception_v3_preprocessing(my_image)\n",
    "my_image = np.expand_dims(my_image, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "\n",
    "  # use saver object to load variables from the saved model\n",
    "  saver.restore(sess, \"../checkpoints/inception_v3.ckpt\")\n",
    "  \n",
    "  # print conv1_1 weight itself\n",
    "  conv1_1_w = sess.run(tf.trainable_variables()[0])\n",
    "  \n",
    "  # print feature maps\n",
    "  conv2, conv4, \\\n",
    "  conv5, conv6, \\\n",
    "  conv7 = sess.run([end_points['Conv2d_2b_3x3'],\n",
    "                    end_points['Conv2d_4a_3x3'],\n",
    "                    end_points['Mixed_5d'],\n",
    "                    end_points['Mixed_6e'],\n",
    "                    end_points['Mixed_7c']],\n",
    "                   feed_dict={inputs: my_image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in end_points.keys():\n",
    "  print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_feature_maps(layer, layer_name):\n",
    "  \"\"\"Print all feature maps\n",
    "    This code is borrowed from \"Deep Learning with Python\" (by F. Chollet)\n",
    "  \n",
    "  Args:\n",
    "    layer (4-rank Tensor): feature maps\n",
    "    layer_name (string): name of feature maps\n",
    "    \n",
    "  Returns:\n",
    "    print all feature maps\n",
    "  \"\"\"\n",
    "  num_features = layer.shape[-1]\n",
    "  size = layer.shape[1]\n",
    "  images_per_row = 16\n",
    "  for feature_map in range(num_features):\n",
    "    num_cols = num_features // images_per_row\n",
    "    display_grid = np.zeros((size * num_cols, images_per_row * size))\n",
    "\n",
    "    for col in range(num_cols):\n",
    "      for row in range(images_per_row):\n",
    "        channel_image = layer[0,:,:,col * images_per_row + row]\n",
    "\n",
    "        channel_image -= channel_image.mean()\n",
    "        channel_image /= channel_image.std()\n",
    "        channel_image *= 64\n",
    "        channel_image += 128\n",
    "        channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "\n",
    "        display_grid[col * size : (col + 1) * size,\n",
    "                     row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "  scale = 1. / size\n",
    "  plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                      scale * display_grid.shape[0]))\n",
    "  plt.title(layer_name)\n",
    "  plt.grid(False)\n",
    "  plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print feature maps of `Conv2d_2b_3x3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_all_feature_maps(conv2, 'conv2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print feature maps of `Conv2d_4a_3x3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_all_feature_maps(conv4, 'conv4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print feature maps of `Mixed_5d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_all_feature_maps(conv5, 'conv5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print feature maps of `Mixed_6e`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_all_feature_maps(conv6, 'conv6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print feature maps of `Mixed_7c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_all_feature_maps(conv7, 'conv7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv7.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
