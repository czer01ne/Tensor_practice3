{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Classification by RNN\n",
    "\n",
    "- Creating the **data pipeline** with `tf.data`\n",
    "- Preprocessing word sequences (variable input sequence length) using `padding technique` by `user function (pad_seq)`\n",
    "- Using `tf.nn.embedding_lookup` for getting vector of tokens (eg. word, character)\n",
    "- Training **many to many classification** with `tf.contrib.seq2seq.sequence_loss`\n",
    "- Masking unvalid token with `tf.sequence_mask`\n",
    "- Creating the model as **Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "rnn = tf.contrib.rnn\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare example data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['I', 'feel', 'hungry'],\n",
    "             ['You', 'are', 'a', 'genius'],\n",
    "             ['tensorflow', 'is', 'very', 'difficult'],\n",
    "             ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "             ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "pos = [['pronoun', 'verb', 'adjective'],\n",
    "       ['pronoun', 'verb', 'preposition', 'noun'],\n",
    "       ['noun', 'verb', 'adverb', 'adjective'],\n",
    "       ['noun', 'verb', 'determiner', 'noun', 'preposition', 'adjective', 'noun'],\n",
    "       ['noun', 'verb', 'adverb', 'adjective', 'verb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word dictionary\n",
    "bag_of_words = []\n",
    "for sentence in sentences:\n",
    "  bag_of_words += sentence\n",
    "bag_of_words = list(set(bag_of_words))\n",
    "bag_of_words.sort()\n",
    "bag_of_words = ['<pad>'] + bag_of_words\n",
    "\n",
    "word2idx = {word : idx for idx, word in enumerate(bag_of_words)} # word to index\n",
    "idx2word = [word for word in bag_of_words] # index to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"word2idx: {}\".format(word2idx))\n",
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"idx2word: {}\".format(idx2word))\n",
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos dictionary\n",
    "bag_of_pos = []\n",
    "for item in pos:\n",
    "  bag_of_pos += item\n",
    "bag_of_pos = list(set(bag_of_pos))\n",
    "bag_of_pos.sort()\n",
    "bag_of_pos = ['<pad>'] + bag_of_pos\n",
    "print(\"bag_of_pos: {}\".format(bag_of_pos))\n",
    "\n",
    "pos2idx = {pos : idx for idx, pos in enumerate(bag_of_pos)} # pos to index\n",
    "idx2pos = [pos for pos in bag_of_pos] # index to pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"pos2idx: {}\".format(pos2idx))\n",
    "pos2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"idx2pos: {}\".format(idx2pos))\n",
    "idx2pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pad_seq function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(sequences, max_length, dic):\n",
    "  \"\"\"Padding sequences\n",
    "  Padding a special charcter '<pad>' from the end of sentence to max_length\n",
    "  \n",
    "  Args:\n",
    "    sequences (list of characters): input data\n",
    "    max_length (int): max length for padding\n",
    "    dic (dictionary): char to index\n",
    "  \n",
    "  Returns:\n",
    "    seq_indices (2-rank np.array): \n",
    "    seq_length (1-rank np.array): sequence lengthes of all data\n",
    "  \"\"\"\n",
    "  seq_length, seq_indices = [], []\n",
    "  for sequence in sequences:\n",
    "    seq_length.append(len(sequence))\n",
    "    seq_idx = [dic.get(char) for char in sequence]\n",
    "    seq_idx += (max_length - len(seq_idx)) * [dic.get('<pad>')] # 0 is idx of meaningless token \"<pad>\"\n",
    "    seq_indices.append(seq_idx)\n",
    "  return np.array(seq_indices), np.array(seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 10\n",
    "X_indices, X_length = pad_seq(sequences=sentences, max_length=max_length, dic=word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_indices\")\n",
    "print(X_indices)\n",
    "print(\"X_length\")\n",
    "print(X_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_string = np.array([item + ['<pad>'] * (max_length - len(item)) for item in pos])\n",
    "print(y_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([list(map(lambda el : pos2idx.get(el), item)) for item in y_string])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define SimPosRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosRNN:\n",
    "  def __init__(self, seq_indices, seq_length, labels, num_classes, hidden_dim, max_length, word2idx):\n",
    "    # Data pipeline\n",
    "    with tf.variable_scope('input_layer'):\n",
    "      self._seq_indices = seq_indices\n",
    "      self._seq_length = seq_length\n",
    "      self._labels = labels\n",
    "\n",
    "      one_hot = tf.eye(len(word2idx), dtype=tf.float32)\n",
    "      self._one_hot = tf.get_variable(name='one_hot_embedding',\n",
    "                                      initializer=one_hot,\n",
    "                                      trainable=False) # embedding vector training 안할 것이기 때문\n",
    "      self._seq_embeddings = tf.nn.embedding_lookup(params=self._one_hot,\n",
    "                                                    ids=self._seq_indices)\n",
    "\n",
    "    # bidirectional LSTM cell (many to many)\n",
    "    with tf.variable_scope('rnn_cell'):\n",
    "      cell_fw = rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True)\n",
    "      cell_bw = rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True)\n",
    "      outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw,\n",
    "                                                   self._seq_embeddings,\n",
    "                                                   sequence_length=self._seq_length,\n",
    "                                                   dtype=tf.float32)\n",
    "      concat_outputs = tf.concat([outputs[0], outputs[1]], axis=2)\n",
    "      \n",
    "      weights = tf.get_variable(name='weights', shape=[2 * hidden_dim, num_classes],\n",
    "                                initializer=slim.xavier_initializer())\n",
    "      self._logits = tf.map_fn(lambda elm : tf.matmul(elm, weights), concat_outputs)\n",
    "\n",
    "    with tf.variable_scope('seq2seq_loss'):\n",
    "      masks = tf.sequence_mask(lengths=self._seq_length, maxlen=max_length, dtype=tf.float32)\n",
    "      self.seq2seq_loss = tf.contrib.seq2seq.sequence_loss(logits=self._logits,\n",
    "                                                           targets=self._labels,\n",
    "                                                           weights=masks)\n",
    "\n",
    "    with tf.variable_scope('prediction'):\n",
    "      self._prediction = tf.argmax(input=self._logits,\n",
    "                                   axis=2, output_type=tf.int32)\n",
    "\n",
    "  def predict(self, sess, seq_indices, seq_length):\n",
    "    feed_dict = {self._seq_indices : seq_indices, self._seq_length : seq_length}\n",
    "    return sess.run(self._prediction, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model of SimPosRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter\n",
    "num_classes = len(idx2pos)\n",
    "learning_rate = .003\n",
    "batch_size = 2\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up dataset with `tf.data`\n",
    "\n",
    "#### create input pipeline with `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data pipeline with tf.data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_indices, X_length, y))\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 100)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "seq_indices, seq_length, labels = train_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rnn = PosRNN(seq_indices=seq_indices, seq_length=seq_length,\n",
    "                 labels=labels, num_classes=num_classes,\n",
    "                 hidden_dim=16, max_length=max_length,\n",
    "                 word2idx=word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat training op and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create training op\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(pos_rnn.seq2seq_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.Session()` and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "loss_history = []\n",
    "step = 0\n",
    "for epochs in range(max_epochs):\n",
    "  start_time = time.time()\n",
    "  sess.run(train_iterator.initializer)\n",
    "  \n",
    "  avg_loss = []\n",
    "  while True:\n",
    "    try:\n",
    "      _, loss_ = sess.run([train_op, pos_rnn.seq2seq_loss])\n",
    "      avg_loss.append(loss_)\n",
    "      step += 1\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      #print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "      break\n",
    "\n",
    "  avg_loss_ = np.mean(avg_loss)\n",
    "  loss_history.append(avg_loss_)\n",
    "  \n",
    "  duration = time.time() - start_time\n",
    "  examples_per_sec = batch_size / float(duration)\n",
    "  print(\"epochs: {}, step: {}, loss: {:g}, ({:.2f} examples/sec; {:.3f} sec/batch)\".format(epochs+1, step, avg_loss_, examples_per_sec, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history, label='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pos_rnn.predict(sess=sess, seq_indices=X_indices, seq_length=X_length)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_str = []\n",
    "for example in y_pred:\n",
    "  result_str.append([idx2pos[idx] for idx in example])\n",
    "  \n",
    "for examples in zip(y_string, result_str):\n",
    "  print(\"        Label: \", ' '.join(examples[0]))\n",
    "  print(\"Prediction: \", ' '.join(examples[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
