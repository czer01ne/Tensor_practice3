{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks for CIFAR-10 data\n",
    "\n",
    "* Cifar-10 data를 가지고 자신만의 **convolutional neural networks**를 만들어보자.\n",
    "  * [참고: TensorFlow.org](https://www.tensorflow.org/get_started/mnist/pros)\n",
    "  * [`tf.layers` API](https://www.tensorflow.org/api_docs/python/tf/layers)\n",
    "  * [`tf.contrib.layers` API](https://www.tensorflow.org/api_docs/python/tf/contrib/layers)\n",
    "  * `tf.contrib.slim`에서 각 `layer`의 옵션들을 직접 컨트롤 해보자.\n",
    "  * `l2_regularization`을 사용해보자.\n",
    "  * `batch_norm`을 어떻게 사용하는지 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A own CIFAR-10 classifier.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), (test_data, test_labels) = \\\n",
    "    tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_data = train_data / 255.\n",
    "train_labels = np.asarray(train_labels, dtype=np.int32)\n",
    "\n",
    "test_data = test_data / 255.\n",
    "test_labels = np.asarray(test_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.squeeze(train_labels)\n",
    "test_labels = np.squeeze(test_labels)\n",
    "\n",
    "N = 30000\n",
    "train_data = train_data[0:N]\n",
    "train_labels = train_labels[0:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the CIFAR-10 data\n",
    "\n",
    "* `label = 0`: airplane\n",
    "* `label = 1`: automobile\n",
    "* `label = 2`: bird\n",
    "* `label = 3`: cat\n",
    "* `label = 4`: deer\n",
    "* `label = 5`: dog\n",
    "* `label = 6`: frog\n",
    "* `label = 7`: horse\n",
    "* `label = 8`: ship\n",
    "* `label = 9`: truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 100\n",
    "print(\"label = {}\".format(idx2label[train_labels[index]]))\n",
    "plt.imshow(train_data[index])\n",
    "#plt.gca().grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset with `tf.data`\n",
    "\n",
    "### create input pipeline with `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _central_crop(image, label):\n",
    "  \"\"\"central crop function for simplicity on CIFAR-10 dataset\n",
    "  \n",
    "  Args:\n",
    "    image (3-rank Tensor): [32, 32, 3] for CIFAR-10 data\n",
    "    label (0-rank Tensor): scalar value of corresponding image\n",
    "    \n",
    "  Returns:\n",
    "    image_augmented (3-rank Tensor): [28, 28, 3] image center cropped\n",
    "    label (0-rank Tensor): scalar value of corresponding image\n",
    "  \"\"\"\n",
    "  image_resized = tf.image.central_crop(image, central_fraction=0.85)\n",
    "  return image_resized, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(219)\n",
    "batch_size = 32\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "train_dataset = train_dataset.map(_central_crop)\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)\n",
    "print(train_dataset)\n",
    "\n",
    "# for test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\n",
    "test_dataset = test_dataset.map(_central_crop)\n",
    "test_dataset = test_dataset.batch(batch_size = len(test_data))\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Iterator.from_string_handle의 output_shapes는 default = None이지만 꼭 값을 넣는 게 좋음\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(handle,\n",
    "                                               train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "x, y = iterator.get_next()\n",
    "x = tf.cast(x, dtype = tf.float32)\n",
    "y = tf.cast(y, dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(x):\n",
    "  \"\"\"Model function for CNN.\n",
    "  Args:\n",
    "    x: input images\n",
    "    \n",
    "  Returns:\n",
    "    logits: unnormalized score funtion\n",
    "  \"\"\"\n",
    "  is_training = tf.placeholder(tf.bool)\n",
    "  batch_norm_params = {'decay': 0.9,\n",
    "                       'epsilon': 0.001,\n",
    "                       'is_training': is_training,\n",
    "                       'scope': 'batch_norm'}\n",
    "  l2_decay = 0.0001\n",
    "  \n",
    "  # Input Layer\n",
    "  # CIFAR-10 images are 32x32 pixels, and have RGB color channel\n",
    "  # input image size: [28, 28, 3] for simplicity\n",
    "  assert x.shape[1] == 28  and  x.shape[2] == 28  and  x.shape[3] == 3\n",
    "\n",
    "  with slim.arg_scope([slim.conv2d],\n",
    "                      kernel_size=[3, 3],\n",
    "                      normalizer_fn=slim.batch_norm,\n",
    "                      normalizer_params=batch_norm_params,\n",
    "                      weights_regularizer=slim.l2_regularizer(l2_decay)):\n",
    "    with slim.arg_scope([slim.max_pool2d],\n",
    "                        kernel_size=[2, 2]):\n",
    "      # Convolutional Layer #1\n",
    "      # Input Tensor Shape: [batch_size, 28, 28, 3]\n",
    "      # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "      conv1 = slim.conv2d(inputs=x, num_outputs=32, scope='conv1')\n",
    "\n",
    "      # Pooling Layer #1\n",
    "      # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "      # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "      pool1 = slim.max_pool2d(inputs=conv1, scope='pool1')\n",
    "\n",
    "      # Convolutional Layer #2\n",
    "      # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "      # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "      conv2 = slim.conv2d(inputs=pool1, num_outputs=64, scope='conv2')\n",
    "\n",
    "      # Pooling Layer #2\n",
    "      # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "      # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "      # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "      pool2 = slim.max_pool2d(inputs=conv2, scope='pool2')\n",
    "      \n",
    "  with slim.arg_scope([slim.fully_connected],\n",
    "                      normalizer_fn=slim.batch_norm,\n",
    "                      normalizer_params=batch_norm_params,\n",
    "                      weights_regularizer=slim.l2_regularizer(l2_decay)):\n",
    "\n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    pool2_flat = slim.flatten(inputs=pool2, scope='pool2_flat')\n",
    "\n",
    "    # Fully connected Layer\n",
    "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    fc1 = slim.fully_connected(inputs=pool2_flat, num_outputs=1024, scope='fc1')\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = slim.dropout(inputs=fc1, keep_prob=0.6, is_training=is_training, scope='dropout')\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = slim.fully_connected(inputs=dropout, num_outputs=10, activation_fn=None, scope='logits')\n",
    "  \n",
    "  return logits, is_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, is_training = cnn_model_fn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define cross entropy loss and regularization loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
    "\n",
    "l2_regualrization_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "\n",
    "with tf.name_scope('total_loss'):\n",
    "  total_loss = cross_entropy + l2_regualrization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization update\n",
    "batchnorm_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "# Add dependency to compute batchnorm_updates.\n",
    "with tf.control_dependencies(batchnorm_update_ops):\n",
    "  train_step = tf.train.AdamOptimizer(1e-4).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign `tf.summary.FileWriter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_location = 'graphs/06.cnn.cifar10.regularization'\n",
    "print('Saving graph to: %s' % graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('summaries'):\n",
    "  tf.summary.scalar('loss/cross_entropy', cross_entropy)\n",
    "  tf.summary.scalar('loss/l2_regualrization_loss', l2_regualrization_loss)\n",
    "  tf.summary.scalar('loss/total_loss', total_loss)\n",
    "  tf.summary.image('images', x)\n",
    "  for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.op.name, var)\n",
    "  # merge all summaries\n",
    "  summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tf.Session()` and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train_iterator\n",
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "train_handle = sess.run(train_iterator.string_handle())\n",
    "\n",
    "# Train\n",
    "max_epochs = 1\n",
    "step = 1\n",
    "for epochs in range(max_epochs):\n",
    "  sess.run(train_iterator.initializer)\n",
    "\n",
    "  while True:\n",
    "    try:\n",
    "      start_time = time.time()\n",
    "      _, loss = sess.run([train_step, total_loss],\n",
    "                         feed_dict={handle: train_handle,\n",
    "                                    is_training: True})\n",
    "      if step % 10 == 0:\n",
    "        clear_output(wait=True)\n",
    "        duration = time.time() - start_time\n",
    "        examples_per_sec = batch_size / float(duration)\n",
    "        print(\"epochs: {}, step: {}, loss: {:g}, ({:.2f} examples/sec; {:.3f} sec/batch)\".format(epochs, step, loss, examples_per_sec, duration))\n",
    "        \n",
    "      if step % 200 == 0:\n",
    "        # summary\n",
    "        summary_str = sess.run(summary_op, feed_dict={handle: train_handle, is_training: False})\n",
    "        train_writer.add_summary(summary_str, global_step=step)\n",
    "        \n",
    "      step += 1\n",
    "      \n",
    "      #if step > 100:\n",
    "      #  break\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "      break\n",
    "\n",
    "train_writer.close()\n",
    "print(\"training done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained model\n",
    "\n",
    "* test accuracy: 0.4594 for one epoch using partial training dataset N = 30000 (without regularization)\n",
    "* test accuracy: 0.5714 for one epoch using partial training dataset N = 30000 (with regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_iterator\n",
    "test_iterator = test_dataset.make_initializable_iterator()\n",
    "test_handle = sess.run(test_iterator.string_handle())\n",
    "sess.run(test_iterator.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, acc_op = tf.metrics.accuracy(labels=y, predictions=tf.argmax(logits, 1), name='accuracy')\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "sess.run(acc_op, feed_dict={handle: test_handle, is_training: False})\n",
    "print(\"test accuracy:\", sess.run(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 16\n",
    "batch_index = np.random.choice(len(test_data), size=test_batch_size, replace=False)\n",
    "batch_xs = test_data[batch_index, 2:-2, 2:-2, :]\n",
    "batch_ys = test_labels[batch_index]\n",
    "y_pred, x_aug_ = sess.run([logits, x],\n",
    "                          feed_dict={x: batch_xs, is_training: False})\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "for i, (px, py) in enumerate(zip(batch_xs, y_pred)):\n",
    "  p = fig.add_subplot(4, 8, i+1)\n",
    "  if np.argmax(py) == batch_ys[i]:\n",
    "    p.set_title(\"y^: {}\\ny: {}\".format(idx2label[np.argmax(py)], idx2label[batch_ys[i]]), color='blue')\n",
    "  else:\n",
    "    p.set_title(\"y^: {}\\ny: {}\".format(idx2label[np.argmax(py)], idx2label[batch_ys[i]]), color='red')\n",
    "  p.imshow(px)\n",
    "  p.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 직접 실습\n",
    "\n",
    "* 여러가지 hyper-parameter들을 바꿔가면서 accuracy를 높혀보자"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
