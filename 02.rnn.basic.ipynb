{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 기본예제\n",
    "\n",
    "* 일부 코드 [김성훈 교수님 TensorFlow 강의자료](https://github.com/hunkim/DeepLearningZeroToAll) 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "rnn = tf.contrib.rnn\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for each char in 'hello'\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://cloud.githubusercontent.com/assets/901975/23348727/cc981856-fce7-11e6-83ea-4b187473466b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RNN Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=sess_config) as sess:\n",
    "  with tf.variable_scope('one_cell', reuse=tf.AUTO_REUSE) as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2)\n",
    "    hidden_size = 2\n",
    "    cell = rnn.BasicRNNCell(num_units=hidden_size)\n",
    "    print(\"output size: {}, state size: {}\".format(cell.output_size, cell.state_size))\n",
    "\n",
    "    x_data = np.array([[h]], dtype=np.float32) # x_data = [[[1,0,0,0]]]\n",
    "    print(\"x_data: {}\".format(x_data))\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_, state_ = sess.run([outputs, state])\n",
    "    print(\"outputs: {}\".format(outputs_))\n",
    "    print(\"state: {}\".format(state_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://cloud.githubusercontent.com/assets/901975/23383634/649efd0a-fd82-11e6-925d-8041242743b0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=sess_config) as sess:\n",
    "  with tf.variable_scope('add_sequances', reuse=tf.AUTO_REUSE) as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2). sequence: 5\n",
    "    hidden_size = 2\n",
    "    cell = rnn.BasicRNNCell(num_units=hidden_size)\n",
    "\n",
    "    x_data = np.array([[h, e, l, l, o]], dtype=np.float32)\n",
    "    print(\"x_data shape: {}\".format(x_data.shape))\n",
    "    print(\"x_data: {}\".format(x_data))\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_, state_ = sess.run([outputs, state])\n",
    "    print(\"outputs: \\n{}\".format(outputs_))\n",
    "    print(\"state: {}\".format(state_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [`tf.nn.static_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn)\n",
    "\n",
    "* `tf.nn.static_rnn` low level code\n",
    "\n",
    "```python\n",
    "state = cell.zero_state(...)\n",
    "outputs = []\n",
    "for input_ in inputs:\n",
    "  output, state = cell(input_, state)\n",
    "  outputs.append(output)\n",
    "return (outputs, state)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://cloud.githubusercontent.com/assets/901975/23383681/9943a9fc-fd82-11e6-8121-bd187994e249.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=sess_config) as sess:\n",
    "  with tf.variable_scope('3_batches_LSTM', reuse=tf.AUTO_REUSE) as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
    "    # 3 batches 'hello', 'eolll', 'lleel'\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                       [e, o, l, l, l],\n",
    "                       [l, l, e, e, l]], dtype=np.float32)\n",
    "\n",
    "    hidden_size = 2\n",
    "    cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_, state_ = sess.run([outputs, state])\n",
    "    print(\"outputs: \\n{}\\n\".format(outputs_))\n",
    "    #print(\"memory cell state: \\n{}\".format(state_[0])) # print memory cell\n",
    "    #print(\"hidden cell state: \\n{}\".format(state_[1])) # print hidden state\n",
    "    print(\"memory cell state: \\n{}\".format(state_.c)) # print memory cell\n",
    "    print(\"hidden cell state: \\n{}\".format(state_.h)) # print hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=sess_config) as sess:\n",
    "  with tf.variable_scope('3_batches_GRU', reuse=tf.AUTO_REUSE) as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
    "    # 3 batches 'hello', 'eolll', 'lleel'\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                       [e, o, l, l, l],\n",
    "                       [l, l, e, e, l]], dtype=np.float32)\n",
    "\n",
    "    hidden_size = 2\n",
    "    cell = rnn.GRUCell(num_units=hidden_size)\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_, state_ = sess.run([outputs, state])\n",
    "    print(\"outputs: \\n{}\\n\".format(outputs_))\n",
    "    print(\"hidden cell state: \\n{}\".format(state_)) # print hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=sess_config) as sess:\n",
    "  with tf.variable_scope('3_batches_dynamic_length', reuse=tf.AUTO_REUSE) as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (5). sequence: 5, batch 3\n",
    "    # 3 batches 'hello', 'eolll', 'lleel'\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                       [e, o, l, l, l],\n",
    "                       [l, l, e, e, l]], dtype=np.float32)\n",
    "\n",
    "    hidden_size = 2\n",
    "    cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_data, sequence_length=[5,3,4], dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_, state_ = sess.run([outputs, state])\n",
    "    print(\"outputs: \\n{}\\n\".format(outputs_))\n",
    "    print(\"memory cell state: \\n{}\".format(state_.c)) # print memory cell\n",
    "    print(\"hidden cell state: \\n{}\".format(state_.h)) # print hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=sess_config) as sess:\n",
    "  with tf.variable_scope('initial_state', reuse=tf.AUTO_REUSE) as scope:\n",
    "    batch_size = 3\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                      [e, o, l, l, l],\n",
    "                      [l, l, e, e, l]], dtype=np.float32)\n",
    "\n",
    "    # One cell RNN input_dim (4) -> output_dim (5). sequence: 5, batch: 3\n",
    "    hidden_size=2\n",
    "    cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_data,\n",
    "                                       initial_state=initial_state,\n",
    "                                       dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_, state_ = sess.run([outputs, state])\n",
    "    print(\"outputs: \\n{}\\n\".format(outputs_))\n",
    "    print(\"memory cell state: \\n{}\".format(state_.c)) # print memory cell\n",
    "    print(\"hidden cell state: \\n{}\".format(state_.h)) # print hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new input data\n",
    "batch_size=3\n",
    "sequence_length=5\n",
    "input_dim=3\n",
    "\n",
    "x_data = np.arange(45, dtype=np.float32).reshape(batch_size, sequence_length, input_dim)\n",
    "print(x_data)  # [batch, sequence_length, input_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-directional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bi-directional rnn](../figures/RNN-bidirectional.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-directional RNN with basic RNN Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=sess_config) as sess:\n",
    "  with tf.variable_scope('bi-directional_RNN', reuse=tf.AUTO_REUSE) as scope:\n",
    "    # bi-directional rnn\n",
    "    cell_fw = rnn.BasicRNNCell(num_units=4)\n",
    "    cell_bw = rnn.BasicRNNCell(num_units=4)\n",
    "\n",
    "    # x_data.shape = (3, 5, 3) [batch, sequence_length, input_dim]\n",
    "    # outputs.shape = two element tuple of (3, 5, 4) [batch, sequence_length, input_dim] shape\n",
    "        # outputs[0]: cell_fw, outputs[1]: cell_bw\n",
    "    # state.shape = two element tuple (3, 4) [batch, sequence_length, input_dim]\n",
    "        # states[0]: cell_fw, states[1]: cell_bw\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, x_data,\n",
    "                                                      sequence_length=[2, 3, 1],\n",
    "                                                      dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_, states_ = sess.run([outputs, states])\n",
    "    print(\"foward outputs: shape {}\\n{}\\n\".format(outputs_[0].shape, outputs_[0]))\n",
    "    print(\"forward hidden cell state: \\n{}\\n\".format(states_[0]))\n",
    "    print(\"backward outputs: shape {}\\n{}\\n\".format(outputs_[1].shape, outputs_[1]))\n",
    "    print(\"backward hidden cell state: \\n{}\".format(states_[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-directional RNN with basic LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=sess_config) as sess:\n",
    "  with tf.variable_scope('bi-directional_LSTM', reuse=tf.AUTO_REUSE) as scope:\n",
    "    # bi-directional rnn\n",
    "    cell_fw = rnn.BasicLSTMCell(num_units=4, state_is_tuple=True)\n",
    "    cell_bw = rnn.BasicLSTMCell(num_units=4, state_is_tuple=True)\n",
    "\n",
    "    # x_data.shape = (3, 5, 3) [batch, sequence_length, input_dim]\n",
    "    # outputs.shape = two element tuple of (3, 5, 4) [batch, sequence_length, input_dim] shape\n",
    "        # outputs[0]: cell_fw, outputs[1]: cell_bw\n",
    "    # state.shape = two element tuple (3, 4) [batch, sequence_length, input_dim]\n",
    "        # states[0]: cell_fw, states[1]: cell_bw\n",
    "    outputs, state = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, x_data,\n",
    "                                                      sequence_length=[2, 3, 1],\n",
    "                                                      dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_, state_ = sess.run([outputs, state])\n",
    "    print(\"foward outputs: shape: {}\\n{}\\n\".format(outputs_[0].shape, outputs_[0]))\n",
    "    print(\"forward memory cell state: \\n{}\".format(state_[0].c))\n",
    "    print(\"forward hidden cell state: \\n{}\\n\".format(state_[0].h))\n",
    "    print(\"backward outputs: shape: {}\\n{}\\n\".format(outputs_[1].shape, outputs_[1]))\n",
    "    print(\"backward memory cell state: \\n{}\".format(state_[1].c))\n",
    "    print(\"backward hidden cell state: \\n{}\".format(state_[1].h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multi-layer rnn](../figures/Multi-layer_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer RNN with basic RNN Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=sess_config) as sess:\n",
    "  with tf.variable_scope('MultiRNN_RNN', reuse=tf.AUTO_REUSE) as scope:\n",
    "    # MultiLayer RNN\n",
    "    num_layers = 3\n",
    "    multi_cells = rnn.MultiRNNCell([rnn.BasicRNNCell(4) for _ in range(num_layers)])\n",
    "\n",
    "    outputs, states = tf.nn.dynamic_rnn(multi_cells, x_data,\n",
    "                                        sequence_length=[2, 3, 1],\n",
    "                                        dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_, states_ = sess.run([outputs, states])\n",
    "    print(\"outputs: shape: {}\\n{}\\n\".format(outputs_.shape, outputs_))\n",
    "    print(\"Number of Layers: {}\".format(len(states_))) \n",
    "    for i in range(num_layers):\n",
    "      print(\"Layer {} hidden cell state: \\n{}\\n\".format(i+1, states_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer RNN with basic LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=sess_config) as sess:\n",
    "  with tf.variable_scope('MultiRNN_LSTM', reuse=tf.AUTO_REUSE) as scope:\n",
    "    # MultiLayer RNN\n",
    "    def lstm_cell(hidden_size):\n",
    "      cell = rnn.BasicLSTMCell(\n",
    "          num_units=hidden_size, state_is_tuple=True)\n",
    "      return cell\n",
    "\n",
    "    num_layers = 3\n",
    "    multi_cells = rnn.MultiRNNCell([lstm_cell(4) for _ in range(num_layers)],\n",
    "                                   state_is_tuple=True)\n",
    "    outputs, states = tf.nn.dynamic_rnn(multi_cells, x_data,\n",
    "                                        sequence_length=[2, 3, 1],\n",
    "                                        dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_, states_ = sess.run([outputs, states])\n",
    "    print(\"outputs: shape: {}\\n{}\\n\".format(outputs_.shape, outputs_))\n",
    "    print(\"Number of Layers: {}\".format(len(states_))) \n",
    "    for i in range(num_layers):\n",
    "      print(\"Layer {} memory cell state: \\n{}\\n\".format(i+1, states_[i].c))\n",
    "      print(\"Layer {} hidden cell state: \\n{}\\n\".format(i+1, states_[i].h))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
