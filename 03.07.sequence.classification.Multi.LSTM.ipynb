{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence classification by RNN\n",
    "\n",
    "- Creating the **data pipeline** with `tf.data`\n",
    "- Preprocessing word sequences (variable input sequence length) using `padding technique` by `user function (pad_seq)`\n",
    "- Using `tf.nn.embedding_lookup` for getting vector of tokens (eg. word, character)\n",
    "- Creating the model as **Class**\n",
    "- Reference\n",
    "    - https://github.com/golbin/TensorFlow-Tutorials/blob/master/10%20-%20RNN/02%20-%20Autocomplete.py\n",
    "    - https://github.com/aisolab/TF_code_examples_for_Deep_learning/blob/master/Tutorial%20of%20implementing%20Sequence%20classification%20with%20RNN%20series.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "rnn = tf.contrib.rnn\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['good', 'bad', 'amazing', 'so good', 'bull shit', 'awesome', 'how dare', 'very much', 'nice']\n",
    "y = np.array([[1.,0.], [0.,1.], [1.,0.], [1.,0.], [0.,1.], [1.,0.], [0.,1.], [1.,0.], [1.,0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character quantization\n",
    "char_space = string.ascii_lowercase \n",
    "char_space = char_space + ' ' + '*' # '*' means padding token\n",
    "print(\"char_space: {}\".format(char_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char = [char for char in char_space]\n",
    "print(\"idx2char: {}\".format(idx2char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {char : idx for idx, char in enumerate(char_space)}\n",
    "print(\"char2idx: {}\".format(char2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pad_seq function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(sequences, max_length, dic):\n",
    "  \"\"\"Padding sequences\n",
    "  \n",
    "  Args:\n",
    "    sequences (list of characters): input data\n",
    "    max_length (int): max length for padding\n",
    "    dic (dictionary): char to index\n",
    "  \n",
    "  Returns:\n",
    "    seq_indices (2-rank np.array): \n",
    "    seq_length (1-rank np.array): sequence lengthes of all data\n",
    "  \"\"\"\n",
    "  seq_length, seq_indices = [], []\n",
    "  for sequence in sequences:\n",
    "    seq_length.append(len(sequence))\n",
    "    seq_idx = [dic.get(char) for char in sequence]\n",
    "    seq_idx += (max_length - len(seq_idx)) * [dic.get('*')] # 27 is idx of meaningless token \"*\"\n",
    "    seq_indices.append(seq_idx)\n",
    "  return np.array(seq_indices), np.array(seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply pad_seq function to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 10\n",
    "X_indices, X_length = pad_seq(sequences=words, max_length=max_length, dic=char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_indices\")\n",
    "print(X_indices)\n",
    "print(\"X_length\")\n",
    "print(X_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CharRNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "  def __init__(self, seq_indices, seq_length, labels, num_classes, hidden_dims, dic):\n",
    "    # data pipeline\n",
    "    with tf.variable_scope('input_layer'):\n",
    "      self._seq_indices = seq_indices\n",
    "      self._seq_length = seq_length\n",
    "      self._labels = labels\n",
    "\n",
    "      one_hot = tf.eye(len(dic), dtype=tf.float32)\n",
    "      self._one_hot = tf.get_variable(name='one_hot_embedding',\n",
    "                                      initializer=one_hot,\n",
    "                                      trainable=False) # embedding vector training 안할 것이기 때문\n",
    "      self._seq_embeddings = tf.nn.embedding_lookup(params=self._one_hot,\n",
    "                                                    ids=self._seq_indices)\n",
    "\n",
    "    # MultiLayer LSTM cell\n",
    "    with tf.variable_scope('multi_lstm_cell'):\n",
    "      multi_cells = rnn.MultiRNNCell([rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True) \\\n",
    "                                      for hidden_dim in hidden_dims])\n",
    "      \n",
    "      _, states = tf.nn.dynamic_rnn(cell=multi_cells, inputs=self._seq_embeddings,\n",
    "                                    sequence_length=self._seq_length, dtype=tf.float32)\n",
    "\n",
    "    with tf.variable_scope('output_layer'):\n",
    "      self._logits = slim.fully_connected(inputs=states[-1].h,\n",
    "                                          num_outputs=num_classes,\n",
    "                                          activation_fn=None)\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "      self.loss = tf.losses.softmax_cross_entropy(onehot_labels=self._labels,\n",
    "                                                  logits=self._logits)\n",
    "\n",
    "    with tf.variable_scope('prediction'):\n",
    "      self._prediction = tf.argmax(input=self._logits, axis=-1, output_type=tf.int32)\n",
    "\n",
    "  def predict(self, sess, seq_indices, seq_length):\n",
    "    feed_dict = {self._seq_indices : seq_indices, self._seq_length : seq_length}\n",
    "    return sess.run(self._prediction, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model of CharRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "num_classes = 2\n",
    "learning_rate = 0.003\n",
    "batch_size = 2\n",
    "max_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_indices: \\n{}\".format(X_indices))\n",
    "print(\"X_length: {}\".format(X_length))\n",
    "print(\"y: \\n{}\".format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up dataset with `tf.data`\n",
    "\n",
    "#### create input pipeline with `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data pipeline with tf.data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_indices, X_length, y))\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 100)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "seq_indices, seq_length, labels = train_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_rnn = CharRNN(seq_indices=seq_indices, seq_length=seq_length,\n",
    "                   labels=labels, num_classes=num_classes,\n",
    "                   hidden_dims=[32, 16], dic=char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat training op and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create training op\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(char_rnn.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.Session()` and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "loss_history = []\n",
    "step = 0\n",
    "for epochs in range(max_epochs):\n",
    "  start_time = time.time()\n",
    "  sess.run(train_iterator.initializer)\n",
    "  \n",
    "  avg_loss = []\n",
    "  while True:\n",
    "    try:\n",
    "      _, loss_ = sess.run([train_op, char_rnn.loss])\n",
    "      avg_loss.append(loss_)\n",
    "      step += 1\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      #print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "      break\n",
    "\n",
    "  avg_loss_ = np.mean(avg_loss)\n",
    "  loss_history.append(avg_loss_)\n",
    "  \n",
    "  duration = time.time() - start_time\n",
    "  examples_per_sec = batch_size / float(duration)\n",
    "  print(\"epochs: {}, step: {}, loss: {:g}, ({:.2f} examples/sec; {:.3f} sec/batch)\".format(epochs+1, step, avg_loss_, examples_per_sec, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_history, label='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = char_rnn.predict(sess=sess, seq_indices=X_indices, seq_length=X_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.mean(y_pred==np.argmax(y, axis=-1))\n",
    "print('training accuracy: {:.2%}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
